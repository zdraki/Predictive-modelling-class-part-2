
\documentclass[12pt]{amsart}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\geometry{a4paper} % or letter or a5paper or ... etc
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\graphicspath{ {images/} }
 



\title{}
\author{}
\date{}


%%% BEGIN DOCUMENT
\begin{document}
\newpage

\maketitle{{\bf Lecture 1 Summary: July 27 2016.}}

{\bf Introduction to Probability }\\

{\bf Overview:}\\

%\begin{itemize}
%\item Random experiment
%\item Sample space (set of possible outcomes)
%\item Sample point (element of the sample space)
%\item Event ( any subset of the sample space)
%\item Probability of a an event (number in $[0,1]$)
%\item Random variable (characteristic of the experiment)
%\end{itemize}





\begin{enumerate}
\item  Random Variables
\item Probability distributions
\item Expected value
\item Probability (Joint, Marginal, and Conditional)
\item Probability of a an event (number in $[0,1]$)
\item Independence
\end{enumerate}

{\bf What is Probability?} - A measure of how likely an event is.\\

The probability $P$ of some event $A$, denoted $P(A)$, is usually defined such that $P(\cdot)$ satisfies the Kolmogorov axioms, named after the Russian mathematician Andrey Kolmogorov:\\

\begin {enumerate}
\item $0 \leq P(A) \leq 1$
\item $P($not $A)=P(A^{\mathrm{c}})=1-P(A)$ ;  either $A$ happens or not
\item $P(A$ or $B)=P(A\cup B)=P(A)+P(B)-P(A\cap B)$\\
{\bf If} events $A$ and $B$ are mutually exclusive; (the realization of $A$ prevents the realization of $B$ and vice versa), we have that $P(A$ or $B)=P(A\cup B)=P(A)+P(B)$ since $P(A\cap B)=0$
\end{enumerate}



\begin{figure}[t]
\includegraphics[width=\textwidth]{treeimage}
\caption{probability tree}
\end{figure}

Let's take a look at figure 1 on page 2:
We can talk about Conditional  and Joint Probability.\\
\begin{enumerate}
\item For women who do not get screened, the probability of dying from breast cancer is $4/15$.
\item For women who get screened, the probability of dying from breast cancer is  $3/15$.
\end{enumerate}

{\bf Conditional probability of $A$ given $B$} is defined as the ratio\\
$P(A|B)=\displaystyle\frac{P(A\cap B)}{P(B)}=\displaystyle\frac{P(AB)}{P(B)}$ $(1)$.\\
Somewhat intuitively in order want to find out how often $A$ occurs, given that we know $B$ has occurred, we need to find the occurrences of $A$ and divide by the total number of possibilities. Since $B$ occurred, the occurrences of $A$ are exactly those situations in which both $A$ and $B$ occurred ($A\cap B$) , and since we're assuming $B$ occurred, the total number of possibilities are reduced to only those where $(1)$ holds since the total number of possibilities in the expressions  $P(AB)$, $P(B)$ cancel.\\

% Enhance it with prof Scott's comments
Note: Conditional probability is {\bf not symmetric}, meaning that  ${\bf P(A|B)\neq P(B|A)}$. This can also be illustrated by the following example.
If $A$ is the event of a person training hard and $B$ the event of a person being an MBA player. The common sense agrees in that given event $B$; a person being an MBA player, the probability that this person is training hard $P(A|B)$ is almost one, whereas given event $A$ a person practicing hard, the probability that this person is an MBA player $P(B|A)$ is basically zero.\\
{\bf Joint probability} $P(AB)$ of $A$ and $B$ is the probability of both events happening.\\
For example in the probability tree above, the probability of a woman developing breast cancer and surviving is $\frac{12}{200}$ (or $\frac{15}{200}\cdot \frac{12}{15}$).\\

So: conditional probability and joint probability are connected by the formula $P(A|B)=\frac{P(AB)}{P(B)}$.\\


{\bf Terminologies}:\\

{\bf Random variable:} We can classify as random variables practically anything we are not sure about. For example the value of a stock index tomorrow, the value of a retirement portfolio in 35 years or the name of the next person that walks by.\\

We can describe the uncertainty about the random variables using a probability distribution.
{\bf Probability distribution} most intuitively is a list of possible outcomes accompanied by their probabilities, we could think of it as a representation of our uncertainty variable. \\

The list of possible outcomes is called {\bf sample space}. The sample space can be {\bf categorical} (heads/tails), {\bf discrete} (anything that is countable, for example the integers from one to 6 ) or {\bf continuous} (anything that can be between two real numbers).

{\bf Expected value} $E(X)=\displaystyle{\sum_{i=1}^nx_iP(X=x_i)}$  is the weighted average of the possible outcomes ($x_i$ is the $i$th element of the sample space) where the weights are the probabilities where. In the case of a continuous random variable, an integral is used in the place of sum. \\

{\bf Joint Distributions and Marginal Probabilities}:
Remember the World War II Example-story:\\
The army was looking for patterns to learn where to reinforce air bombers. The personnel reviewed some data and found some interesting patterns. At first they were mistaken because they used the wrong conditional probability than the one they needed and also assumed it was symmetric.

Let's go over quickly this example in order to remember the joint distributions and marginal probabilities we discussed about.
\newpage

  \begin{figure}[t]
\includegraphics[width=7cm, height=4cm]{image1}
\caption{}
\end{figure}
 
 We can convert the data listed in figure 2 to a probability distribution by making the percentages to sum to 1.\\
 Then we will get the following table illustrated in figure 3 above.\\
 These patterns runfortunately were of the planes that actually returned;not of the planes that were taken down.We can see Wald's
 reconstruction of the joint distribution. Wald argued that the Navy was using the wrong probability and he was right!
  \begin{figure}[t]
\includegraphics[width=7cm, height=4cm]{image2}
\caption{}
\end{figure}
 
 \begin{figure}[t]
\includegraphics[width=7cm, height=4cm]{image3}
\caption{}
\end{figure}
\newpage


{\bf Marginal Probabilities}:

Again, in the same context, we can compute the marginal probabilities through summing across the relevant margin. (See figure 5 )
That is how marginal probability got its name.

 \begin{figure}[t]
\includegraphics[width=7cm, height=4cm]{image4}
\caption{}
\end{figure}

Conditional Probability (think in term of frequencies rather than probabilities) and Independence :\\
We have already defined formally the conditional probability $P(A|B)=\displaystyle\frac{P(A\cap B)}{P(B)}$. 
In the context of the previous example, \\

 \begin{equation} 
\begin{split}
P(\text{returns}|\text{ engine hit} )& = \frac{\text {Fraction taking engine hit and returning safely}}{\text{Fraction taking engine hit}}\\
& = \frac{53/800}{110/800} \\
& = \frac{0.066}{0.137}\\
& \approx 0.48
\end{split}
\end{equation}


{\bf Independence}:\\

$A$ and $B$ are independent if $P(A)=P(A|B)=P(A|B^{\mathrm{c}})$. \\
That means knowing whether $B$ happened does not affect the probability of $A$ happening.\\
Be careful, a lot of fallacies can be boiled down to misunderstandings of this notion. For example:\\

Let $E$: the event that an American works for Enron and $L$: the event that an American works for Lehman Brothers.\\
What is the (joint) probability that an American chosen at random works for both firms? In other words, what is $P(EL)$?\\
If we begin assuming that these two events $E,L$ are independent we might come up with the number $P(EL)=P(E)\cdot P(L)\approx\frac{20,000}{200,000,000}\cdot\frac{26,000}{200,000,000}\approx 1.3\times 10^{-8}$. This result is very alarming. There can't be true that there are only 2 people in the entire country who work in both firms. What led us to this false conclusion was our initial hypothesis about the independence of $L,E$. If they are not assumed independent (and indeed they are not independent of each other, they are part of the same industry), the correct calculation will come from the formula: \\
$P(EL)=P(E)\cdot P(L|E)=\frac{20,000}{200,000,000}\cdot \frac{26,000}{2,000}\approx 1.3\times 10^{-6}$ which yields as an answer one american in a million, which makes much more sense. \\

Bottom line, we must be careful when making assumptions about independence. \\

{\bf Synopsis of the formulas and/or important points:}\\

\begin{enumerate}
\item  $P(A^{\mathrm{c}})=1-P(A)$\\
\item $P(A\cup B)=P(A)+P(B)-P(A\cap B)$ in general, unless $A$ and $B$ are mutually exclusive hence $P(A\cap B)=0$\\
\item $P(A|B)=\displaystyle\frac{P(A\cap B)}{P(B)}=\displaystyle\frac{P(AB)}{P(B)}$\\
\item $E(X)=\displaystyle{\sum_{i=1}^nx_iP(X=x_i)}$\\
\item Joint probability $P(AB)$ of $A$ and $B$ is the probability of both events happening\\
\item Always remember that the probability of a an event is a number in $[0,1]$ !\\
\end{enumerate}




















\end{document}

